{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the data directory\n",
    "abs_dir = os.path.abspath(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(abs_dir, 'scripts/data/cbe_reviews_20250608_162756.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>upvote</th>\n",
       "      <th>date</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>really am happy to this app it is Siple to use everything</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I liked this app. But the User interface is very basic and not attractive at all.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Why don‚Äôt your ATMs support account-to-account transfers like other countries( Kenya, Nigeria , South africa)\"</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is this app problem???</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       review_text  \\\n",
       "0                                                        really am happy to this app it is Siple to use everything   \n",
       "1                                I liked this app. But the User interface is very basic and not attractive at all.   \n",
       "2  \"Why don‚Äôt your ATMs support account-to-account transfers like other countries( Kenya, Nigeria , South africa)\"   \n",
       "3                                                                                      what is this app problem???   \n",
       "4                                                                     the app is proactive and a good connections.   \n",
       "\n",
       "   rating  upvote        date                    bank_name       source  \n",
       "0       5       0  2025-06-07  Commercial Bank of Ethiopia  Google Play  \n",
       "1       2       0  2025-06-07  Commercial Bank of Ethiopia  Google Play  \n",
       "2       4       0  2025-06-06  Commercial Bank of Ethiopia  Google Play  \n",
       "3       1       0  2025-06-05  Commercial Bank of Ethiopia  Google Play  \n",
       "4       5       0  2025-06-05  Commercial Bank of Ethiopia  Google Play  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_text    object\n",
       "rating          int64\n",
       "upvote          int64\n",
       "date           object\n",
       "bank_name      object\n",
       "source         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check types \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>upvote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.110500</td>\n",
       "      <td>8.488250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.474562</td>\n",
       "      <td>82.261194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3025.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating       upvote\n",
       "count  4000.000000  4000.000000\n",
       "mean      4.110500     8.488250\n",
       "std       1.474562    82.261194\n",
       "min       1.000000     0.000000\n",
       "25%       4.000000     0.000000\n",
       "50%       5.000000     1.000000\n",
       "75%       5.000000     1.000000\n",
       "max       5.000000  3025.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_text    0\n",
       "rating         0\n",
       "upvote         0\n",
       "date           0\n",
       "bank_name      0\n",
       "source         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate Entries\n",
    "\n",
    "When I check for duplicate entries in general, I find 81 duplicate rows. However, when I define duplicates based on both review_text and rating, the number increases to 1,162 rows. This indicates that the most influential factors in identifying duplicates are the review text and the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data duplicated: 1162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>upvote</th>\n",
       "      <th>date</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ok</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-02-09</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Commercial Bank of Ethiopia</td>\n",
       "      <td>Google Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_text  rating  upvote        date                    bank_name  \\\n",
       "15          good       5       0  2025-06-04  Commercial Bank of Ethiopia   \n",
       "74          good       5       0  2025-05-23  Commercial Bank of Ethiopia   \n",
       "76          good       5       0  2025-05-23  Commercial Bank of Ethiopia   \n",
       "86            ok       5       0  2025-05-22  Commercial Bank of Ethiopia   \n",
       "103         best       5       0  2025-05-21  Commercial Bank of Ethiopia   \n",
       "...          ...     ...     ...         ...                          ...   \n",
       "3381        Good       5       1  2024-02-11  Commercial Bank of Ethiopia   \n",
       "3396        Good       5       1  2024-02-11  Commercial Bank of Ethiopia   \n",
       "3409        Good       5       1  2024-02-09  Commercial Bank of Ethiopia   \n",
       "3641        Good       5       1  2024-01-05  Commercial Bank of Ethiopia   \n",
       "3682        Good       5       1  2023-12-30  Commercial Bank of Ethiopia   \n",
       "\n",
       "           source  \n",
       "15    Google Play  \n",
       "74    Google Play  \n",
       "76    Google Play  \n",
       "86    Google Play  \n",
       "103   Google Play  \n",
       "...           ...  \n",
       "3381  Google Play  \n",
       "3396  Google Play  \n",
       "3409  Google Play  \n",
       "3641  Google Play  \n",
       "3682  Google Play  \n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicated data\n",
    "dublicated_data = df.duplicated(subset=['review_text', 'rating'], keep=False).sum()\n",
    "print(f'data duplicated: {dublicated_data}')\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated data\n",
    "df.drop_duplicates(subset=['review_text', 'rating'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_text            object\n",
       "rating                  int64\n",
       "upvote                  int64\n",
       "date           datetime64[ns]\n",
       "bank_name              object\n",
       "source                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change date format from object to 'YYYY-MM-DD' format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2988, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\elsha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#distilBERT-base-uncased,': Expected package name at the start of dependency specifier\n",
      "    #distilBERT-base-uncased,\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\elsha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers #distilBERT-base-uncased, which helps to classify the reviews\n",
    "!pip install huggingface_hub[hf_xet] # for downloading the model from Hugging Face, for the purpose of sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                       review_text  \\\n",
      "0                                                        really am happy to this app it is Siple to use everything   \n",
      "1                                I liked this app. But the User interface is very basic and not attractive at all.   \n",
      "2  \"Why don‚Äôt your ATMs support account-to-account transfers like other countries( Kenya, Nigeria , South africa)\"   \n",
      "3                                                                                      what is this app problem???   \n",
      "4                                                                     the app is proactive and a good connections.   \n",
      "\n",
      "  sentiment_label  sentiment_score  \\\n",
      "0        POSITIVE         0.998870   \n",
      "1        NEGATIVE         0.999684   \n",
      "2        NEGATIVE         0.996465   \n",
      "3        NEGATIVE         0.999623   \n",
      "4        POSITIVE         0.999868   \n",
      "\n",
      "                                     sentiment_result  \n",
      "0  {'label': 'POSITIVE', 'score': 0.9988697171211243}  \n",
      "1  {'label': 'NEGATIVE', 'score': 0.9996840953826904}  \n",
      "2  {'label': 'NEGATIVE', 'score': 0.9964652061462402}  \n",
      "3  {'label': 'NEGATIVE', 'score': 0.9996225833892822}  \n",
      "4  {'label': 'POSITIVE', 'score': 0.9998679161071777}  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline,  creates a pipeline for a specific NLP task. In this case, the task is sentiment analysis.\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\") \n",
    "#pipline is used to perform sentiment analysis on the review_text column\n",
    "\n",
    "# Apply sentiment to review_text column\n",
    "df['sentiment_result'] = df['review_text'].astype(str).apply(lambda x: sentiment_pipeline(x)[0]) \n",
    "#the above line applies the sentiment analysis pipeline to each review text and stores the result in a new column 'sentiment_result'\n",
    "#astype(str) ensures that the review_text is treated as a string, even if it contains null values or other types.\n",
    "#sentiment_pipeline(x)[0] returns the first result of the sentiment analysis, which is a dictionary containing the sentiment label and score.\n",
    "\n",
    "df['sentiment_label'] = df['sentiment_result'].apply(lambda x: x['label'])\n",
    "#the above line extracts the sentiment label (e.g., 'POSITIVE' or 'NEGATIVE') from the sentiment result and stores it in a new column 'sentiment_label'\n",
    "#apply(lambda x: x['label']) is used to extract the label from each sentiment result.\n",
    "\n",
    "df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x['score'])\n",
    "#the above line extracts the sentiment score (a numerical value indicating the confidence of the sentiment) from the sentiment result and stores it in a new column 'sentiment_score'\n",
    "\n",
    "print(df[['review_text', 'sentiment_label', 'sentiment_score',\"sentiment_result\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     bank_name  rating  sentiment_score\n",
      "0  Commercial Bank of Ethiopia       1         0.976435\n",
      "1  Commercial Bank of Ethiopia       2         0.963838\n",
      "2  Commercial Bank of Ethiopia       3         0.959347\n",
      "3  Commercial Bank of Ethiopia       4         0.953669\n",
      "4  Commercial Bank of Ethiopia       5         0.960054\n"
     ]
    }
   ],
   "source": [
    "agg_mean = df.groupby([\"bank_name\", \"rating\"])[\"sentiment_score\"].mean().reset_index()\n",
    "#this line groups the DataFrame by 'bank_name' and 'rating', calculates the mean sentiment score for each group, and resets the index to create a new DataFrame with the results.\n",
    "#reset_index() is used to convert the grouped DataFrame back into a regular DataFrame format.\n",
    "\n",
    "print(arg_mean.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     bank_name  rating  sentiment_score\n",
      "0  Commercial Bank of Ethiopia       1         0.976435\n",
      "1  Commercial Bank of Ethiopia       2         0.963838\n",
      "2  Commercial Bank of Ethiopia       3         0.959347\n",
      "3  Commercial Bank of Ethiopia       4         0.953669\n",
      "4  Commercial Bank of Ethiopia       5         0.960054\n"
     ]
    }
   ],
   "source": [
    "# Rename the column for clarity (optional)\n",
    "agg_mean.rename(columns={'sentiment_score': 'avg_sentiment_score'}, inplace=True)\n",
    "#this line renames the 'sentiment_score' column to 'avg_sentiment_score' for clarity, indicating that it represents the average sentiment score for each group.\n",
    "\n",
    "# agg_df = agg_mean.rename(columns={'sentiment_score': 'average_sentiment_score'}, inplace=True)\n",
    "print(arg_mean.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thematic Analysis\n",
    "A theme refers to a recurring concept or topic within user reviews. For this challenge, themes will help summarize user feedback into actionable categories for the banks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Extraction & Manual/Rule-Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keywords: 100\n",
      "Keywords: ['account' 'after' 'all' 'am' 'amazing' 'an' 'and' 'app' 'application'\n",
      " 'apps' 'are' 'as' 'at' 'bank' 'banking' 'be' 'best' 'but' 'by' 'can']\n"
     ]
    }
   ],
   "source": [
    "verctorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Fit and transform the review_text column to create a TF-IDF matrix\n",
    "\n",
    "tfidf_verctorized = verctorizer.fit_transform(df['review_text'].astype(str))\n",
    "\n",
    "keywords = verctorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of keywords: {len(keywords)}\")\n",
    "\n",
    "print(f\"Keywords: {keywords[:20]}\")  # Display the first 10 keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank Commercial Bank of Ethiopia\n",
      "bank_keywords: {'Commercial Bank of Ethiopia': array(['access', 'account', 'add', 'amazing', 'app', 'application',\n",
      "       'apps', 'available', 'bad', 'balance', 'bank', 'banking', 'banks',\n",
      "       'best', 'better', 'birr', 'branch', 'cbe', 'commercial',\n",
      "       'customer', 'days', 'developer', 'doesn', 'don', 'easy', 'error',\n",
      "       'ethiopia', 'excellent', 'experience', 'fast', 'fix', 'friendly',\n",
      "       'good', 'great', 'help', 'history', 'important', 'interesting',\n",
      "       'issue', 'just', 'life', 'like', 'love', 'make', 'message',\n",
      "       'mobile', 'mode', 'money', 'need', 'needs', 'network', 'new',\n",
      "       'nice', 'ok', 'option', 'options', 'payment', 'phone', 'poor',\n",
      "       'problem', 'properly', 'really', 'reason', 'receipt', 'recent',\n",
      "       'reliable', 'says', 'screenshot', 'security', 'seen', 'send',\n",
      "       'service', 'simple', 'statement', 'telebirr', 'thank', 'thanks',\n",
      "       'thing', 'time', 'times', 'transaction', 'transactions',\n",
      "       'transfer', 'try', 'update', 'use', 'used', 'useful', 'user',\n",
      "       'using', 'verification', 'version', 'way', 'work', 'working',\n",
      "       'worst', 'wow', '·â†·å£·àù', '·äê·ãç', '·ä•·äì'], dtype=object)}\n",
      "Commercial Bank of Ethiopia: ['Feature Requests', 'Account Access Issues', 'Transaction Performance', 'User Interface & Experience', 'Customer Support']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bank_keywords = {}\n",
    "# grouped_by_bank = df.groupby('bank_name')\n",
    "# print(\" grouped by bank name  \",grouped_by_bank) # this line counts the number of reviews for each bank and displays the result.\n",
    "\n",
    "\n",
    "\n",
    "for bank, group in df.groupby('bank_name'): # grouping the DataFrame by 'bank_name', output is a dictionary-like object where each key is a bank name and each value is a DataFrame containing the reviews for that bank.\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "    print(\"bank\", bank)  # Display the bank name and its corresponding group of reviews\n",
    "    #print( \"group\", group[\"review_text\"].head(5))  # Display the first 5 reviews for the current bank\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(group['review_text'].astype(str)) # this line fits the TF-IDF vectorizer to the review_text of each bank and transforms it into a TF-IDF matrix.\n",
    "\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    # print(\"keywords for bank:\", bank, \"->\", keywords)  # Display the keywords for each bank\n",
    "    \n",
    "    bank_keywords[bank] = keywords # Store the keywords for each bank in the dictionary,eg. CBE:{keyword1, keyword2, ...}\n",
    "\n",
    "    print(\"bank_keywords:\", bank_keywords)  # Display the dictionary containing keywords for each bank\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Grouping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commercial Bank of Ethiopia: ['Feature Requests', 'Account Access Issues', 'Transaction Performance', 'User Interface & Experience', 'Customer Support']\n"
     ]
    }
   ],
   "source": [
    "theme_counts = themes = {\n",
    "    'Account Access Issues': ['login', 'access', 'account', 'password', 'blocked', 'error'],\n",
    "    'Transaction Performance': ['transaction', 'delay', 'payment', 'processing', 'funds', 'failed'],\n",
    "    'User Interface & Experience': ['app', 'interface', 'design', 'navigation', 'crash', 'slow'],\n",
    "    'Customer Support': ['support', 'agent', 'response', 'feedback', 'service'],\n",
    "    'Feature Requests': ['feature', 'add', 'need', 'want', 'update', 'new']\n",
    "}\n",
    "\n",
    "bank_theme_mapping = {}\n",
    "\n",
    "for bank, keywords in bank_keywords.items():\n",
    "    theme_counts = {theme: 0 for theme in themes}\n",
    "    \n",
    "    for word in keywords:\n",
    "        for theme, word_list in themes.items():\n",
    "            if word in word_list:\n",
    "                theme_counts[theme] += 1\n",
    "    \n",
    "    # Sort and keep top 3‚Äì5 themes\n",
    "    sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True) #x[1] is used to sort the themes based on their counts in descending order.\n",
    "    bank_theme_mapping[bank] = [theme for theme, count in sorted_themes if count > 0][:5]\n",
    "\n",
    "# Display\n",
    "for bank, top_themes in bank_theme_mapping.items():\n",
    "    print(f\"{bank}: {top_themes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Alternative Approach: Use Embedding Similarity (with spaCy)\n",
    "This approach:\n",
    "\n",
    "Uses word embeddings to measure similarity between keywords and theme labels.\n",
    "\n",
    "Dynamically assigns keywords to themes without hardcoding word lists.\n",
    "\n",
    "Works well when keywords are varied or noisy.\n",
    "\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What is spaCy?\n",
    "\n",
    "spaCy is a popular open-source Natural Language Processing (NLP) library in Python.\n",
    "\n",
    "‚úÖ It provides:\n",
    "Tokenization (splitting sentences into words)\n",
    "\n",
    "Part-of-speech tagging (e.g., noun, verb)\n",
    "\n",
    "Named entity recognition (e.g., people, places)\n",
    "\n",
    "Word vectors / embeddings (words as mathematical vectors for similarity)\n",
    "\n",
    "Dependency parsing (grammar structure of a sentence)\n",
    "\n",
    "üîç Why is spaCy useful?\n",
    "You can compare words by meaning using pre-trained word vectors.\n",
    "\n",
    "It allows semantic similarity, like matching \"login\" with \"access issue\".\n",
    "\n",
    "But spaCy is not required for everything ‚Äî you can do topic grouping without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (80.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\elsha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 2.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/13.9 MB 1.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/13.9 MB 1.5 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.0/13.9 MB 1.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.3/13.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 2.1/13.9 MB 1.0 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/13.9 MB 1.0 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.4/13.9 MB 1.0 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.6/13.9 MB 986.7 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.9/13.9 MB 975.6 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.9/13.9 MB 975.6 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 3.1/13.9 MB 966.3 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 3.1/13.9 MB 966.3 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 3.4/13.9 MB 945.2 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 3.7/13.9 MB 932.1 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.7/13.9 MB 932.1 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.9/13.9 MB 910.4 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 4.2/13.9 MB 905.2 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 4.2/13.9 MB 905.2 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 4.5/13.9 MB 903.8 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 4.7/13.9 MB 902.7 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 4.7/13.9 MB 902.7 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 5.0/13.9 MB 901.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.2/13.9 MB 895.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.2/13.9 MB 895.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.5/13.9 MB 871.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.5/13.9 MB 871.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.5/13.9 MB 871.5 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 828.9 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 828.9 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 828.9 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 828.9 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 769.0 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 769.0 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 769.0 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 769.0 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.3/13.9 MB 736.4 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.3/13.9 MB 736.4 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.6/13.9 MB 719.0 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.6/13.9 MB 719.0 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.6/13.9 MB 719.0 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.8/13.9 MB 708.5 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.8/13.9 MB 708.5 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 7.1/13.9 MB 703.6 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 7.1/13.9 MB 703.6 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.9 MB 697.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.9 MB 697.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.9 MB 697.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.9 MB 697.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.6/13.9 MB 673.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.6/13.9 MB 673.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.6/13.9 MB 673.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.6/13.9 MB 673.0 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 638.5 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 604.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 604.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 604.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 604.9 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 604.9 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.4/13.9 MB 576.0 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.7/13.9 MB 548.4 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.7/13.9 MB 548.4 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.7/13.9 MB 548.4 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.7/13.9 MB 548.4 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 8.7/13.9 MB 548.4 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 8.9/13.9 MB 528.3 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 8.9/13.9 MB 528.3 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 8.9/13.9 MB 528.3 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 8.9/13.9 MB 528.3 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 9.2/13.9 MB 518.1 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 9.2/13.9 MB 518.1 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 9.2/13.9 MB 518.1 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 9.4/13.9 MB 513.7 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 9.4/13.9 MB 513.7 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 9.4/13.9 MB 513.7 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 9.7/13.9 MB 512.3 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 9.7/13.9 MB 512.3 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 512.2 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 512.2 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 512.2 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 509.6 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 509.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 10.5/13.9 MB 509.6 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 10.5/13.9 MB 509.6 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 10.5/13.9 MB 509.6 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 10.7/13.9 MB 510.0 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 10.7/13.9 MB 510.0 kB/s eta 0:00:07\n",
      "   ------------------------------- -------- 11.0/13.9 MB 511.8 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 11.0/13.9 MB 511.8 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 11.3/13.9 MB 513.6 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 11.3/13.9 MB 513.6 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 11.3/13.9 MB 513.6 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 11.5/13.9 MB 512.4 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 11.5/13.9 MB 512.4 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 11.8/13.9 MB 513.0 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 11.8/13.9 MB 513.0 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 514.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 514.6 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 516.6 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 516.6 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 12.6/13.9 MB 517.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.6/13.9 MB 517.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.8/13.9 MB 518.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.8/13.9 MB 518.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.8/13.9 MB 518.9 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 13.1/13.9 MB 519.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 13.1/13.9 MB 519.6 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 13.4/13.9 MB 521.3 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 13.4/13.9 MB 521.3 kB/s eta 0:00:02\n",
      "   ---------------------------------------  13.6/13.9 MB 523.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/13.9 MB 525.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 525.6 kB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 493.7 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 493.7 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 493.7 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 479.7 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 479.7 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 479.7 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.0/2.0 MB 461.8 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 461.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 466.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 466.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 466.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.6/2.0 MB 468.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 468.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 468.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/2.0 MB 464.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 457.4 kB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   ------------------------------- ------ 524.3/632.6 kB 558.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 632.6/632.6 kB 538.5 kB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 409.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 409.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 409.6 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 409.6 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 394.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 394.8 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 394.8 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 384.4 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 384.4 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 384.4 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 384.4 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 366.8 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 366.8 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 366.8 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 366.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 343.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 343.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 343.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 335.2 kB/s eta 0:00:00\n",
      "Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.7 MB 305.2 kB/s eta 0:00:41\n",
      "   - -------------------------------------- 0.5/12.7 MB 305.2 kB/s eta 0:00:41\n",
      "   - -------------------------------------- 0.5/12.7 MB 305.2 kB/s eta 0:00:41\n",
      "   - -------------------------------------- 0.5/12.7 MB 305.2 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 0.8/12.7 MB 305.0 kB/s eta 0:00:40\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 1.0/12.7 MB 256.8 kB/s eta 0:00:46\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 242.3 kB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 242.3 kB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 242.3 kB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 242.3 kB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 242.3 kB/s eta 0:00:48\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 242.5 kB/s eta 0:00:47\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 242.5 kB/s eta 0:00:47\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 242.5 kB/s eta 0:00:47\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 242.5 kB/s eta 0:00:47\n",
      "   ---- ----------------------------------- 1.6/12.7 MB 242.5 kB/s eta 0:00:47\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 245.5 kB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 245.5 kB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 245.5 kB/s eta 0:00:45\n",
      "   ----- ---------------------------------- 1.8/12.7 MB 245.5 kB/s eta 0:00:45\n",
      "   ------ --------------------------------- 2.1/12.7 MB 253.1 kB/s eta 0:00:43\n",
      "   ------ --------------------------------- 2.1/12.7 MB 253.1 kB/s eta 0:00:43\n",
      "   ------ --------------------------------- 2.1/12.7 MB 253.1 kB/s eta 0:00:43\n",
      "   ------ --------------------------------- 2.1/12.7 MB 253.1 kB/s eta 0:00:43\n",
      "   ------- -------------------------------- 2.4/12.7 MB 260.6 kB/s eta 0:00:40\n",
      "   ------- -------------------------------- 2.4/12.7 MB 260.6 kB/s eta 0:00:40\n",
      "   ------- -------------------------------- 2.4/12.7 MB 260.6 kB/s eta 0:00:40\n",
      "   -------- ------------------------------- 2.6/12.7 MB 266.8 kB/s eta 0:00:38\n",
      "   -------- ------------------------------- 2.6/12.7 MB 266.8 kB/s eta 0:00:38\n",
      "   -------- ------------------------------- 2.6/12.7 MB 266.8 kB/s eta 0:00:38\n",
      "   -------- ------------------------------- 2.6/12.7 MB 266.8 kB/s eta 0:00:38\n",
      "   --------- ------------------------------ 2.9/12.7 MB 271.0 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 2.9/12.7 MB 271.0 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 2.9/12.7 MB 271.0 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 2.9/12.7 MB 271.0 kB/s eta 0:00:37\n",
      "   --------- ------------------------------ 3.1/12.7 MB 277.5 kB/s eta 0:00:35\n",
      "   --------- ------------------------------ 3.1/12.7 MB 277.5 kB/s eta 0:00:35\n",
      "   --------- ------------------------------ 3.1/12.7 MB 277.5 kB/s eta 0:00:35\n",
      "   --------- ------------------------------ 3.1/12.7 MB 277.5 kB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 280.0 kB/s eta 0:00:34\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 280.0 kB/s eta 0:00:34\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 280.0 kB/s eta 0:00:34\n",
      "   ---------- ----------------------------- 3.4/12.7 MB 280.0 kB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 282.2 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 282.2 kB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 282.2 kB/s eta 0:00:33\n",
      "   ------------ --------------------------- 3.9/12.7 MB 287.8 kB/s eta 0:00:31\n",
      "   ------------ --------------------------- 3.9/12.7 MB 287.8 kB/s eta 0:00:31\n",
      "   ------------ --------------------------- 3.9/12.7 MB 287.8 kB/s eta 0:00:31\n",
      "   ------------- -------------------------- 4.2/12.7 MB 294.0 kB/s eta 0:00:30\n",
      "   ------------- -------------------------- 4.2/12.7 MB 294.0 kB/s eta 0:00:30\n",
      "   ------------- -------------------------- 4.2/12.7 MB 294.0 kB/s eta 0:00:30\n",
      "   -------------- ------------------------- 4.5/12.7 MB 297.9 kB/s eta 0:00:28\n",
      "   -------------- ------------------------- 4.5/12.7 MB 297.9 kB/s eta 0:00:28\n",
      "   -------------- ------------------------- 4.5/12.7 MB 297.9 kB/s eta 0:00:28\n",
      "   -------------- ------------------------- 4.5/12.7 MB 297.9 kB/s eta 0:00:28\n",
      "   -------------- ------------------------- 4.7/12.7 MB 299.3 kB/s eta 0:00:27\n",
      "   -------------- ------------------------- 4.7/12.7 MB 299.3 kB/s eta 0:00:27\n",
      "   -------------- ------------------------- 4.7/12.7 MB 299.3 kB/s eta 0:00:27\n",
      "   -------------- ------------------------- 4.7/12.7 MB 299.3 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 5.0/12.7 MB 302.3 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 5.0/12.7 MB 302.3 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 5.0/12.7 MB 302.3 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 5.0/12.7 MB 302.3 kB/s eta 0:00:26\n",
      "   --------------- ------------------------ 5.0/12.7 MB 302.3 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 297.9 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 297.9 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 297.9 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 297.9 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 297.9 kB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 294.1 kB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 294.1 kB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 294.1 kB/s eta 0:00:25\n",
      "   ----------------- ---------------------- 5.5/12.7 MB 294.1 kB/s eta 0:00:25\n",
      "   ------------------ --------------------- 5.8/12.7 MB 295.6 kB/s eta 0:00:24\n",
      "   ------------------ --------------------- 5.8/12.7 MB 295.6 kB/s eta 0:00:24\n",
      "   ------------------ --------------------- 5.8/12.7 MB 295.6 kB/s eta 0:00:24\n",
      "   ------------------ --------------------- 5.8/12.7 MB 295.6 kB/s eta 0:00:24\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------ --------------------- 6.0/12.7 MB 295.3 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 6.3/12.7 MB 283.1 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 6.6/12.7 MB 278.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 6.6/12.7 MB 278.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 6.6/12.7 MB 278.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 6.6/12.7 MB 278.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 6.6/12.7 MB 278.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 6.8/12.7 MB 277.4 kB/s eta 0:00:22\n",
      "   --------------------- ------------------ 6.8/12.7 MB 277.4 kB/s eta 0:00:22\n",
      "   --------------------- ------------------ 6.8/12.7 MB 277.4 kB/s eta 0:00:22\n",
      "   --------------------- ------------------ 6.8/12.7 MB 277.4 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 279.3 kB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 279.3 kB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 279.3 kB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 282.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 282.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 282.8 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 286.1 kB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 286.1 kB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 7.6/12.7 MB 286.1 kB/s eta 0:00:18\n",
      "   ------------------------ --------------- 7.9/12.7 MB 288.9 kB/s eta 0:00:17\n",
      "   ------------------------ --------------- 7.9/12.7 MB 288.9 kB/s eta 0:00:17\n",
      "   ------------------------ --------------- 7.9/12.7 MB 288.9 kB/s eta 0:00:17\n",
      "   ------------------------- -------------- 8.1/12.7 MB 290.4 kB/s eta 0:00:16\n",
      "   ------------------------- -------------- 8.1/12.7 MB 290.4 kB/s eta 0:00:16\n",
      "   ------------------------- -------------- 8.1/12.7 MB 290.4 kB/s eta 0:00:16\n",
      "   -------------------------- ------------- 8.4/12.7 MB 293.3 kB/s eta 0:00:15\n",
      "   -------------------------- ------------- 8.4/12.7 MB 293.3 kB/s eta 0:00:15\n",
      "   -------------------------- ------------- 8.4/12.7 MB 293.3 kB/s eta 0:00:15\n",
      "   -------------------------- ------------- 8.4/12.7 MB 293.3 kB/s eta 0:00:15\n",
      "   --------------------------- ------------ 8.7/12.7 MB 295.1 kB/s eta 0:00:14\n",
      "   --------------------------- ------------ 8.7/12.7 MB 295.1 kB/s eta 0:00:14\n",
      "   --------------------------- ------------ 8.7/12.7 MB 295.1 kB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 297.8 kB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 297.8 kB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 297.8 kB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 301.0 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 301.0 kB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 304.7 kB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 304.7 kB/s eta 0:00:11\n",
      "   ------------------------------ --------- 9.7/12.7 MB 308.6 kB/s eta 0:00:10\n",
      "   ------------------------------ --------- 9.7/12.7 MB 308.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 10.0/12.7 MB 313.2 kB/s eta 0:00:09\n",
      "   ------------------------------- -------- 10.0/12.7 MB 313.2 kB/s eta 0:00:09\n",
      "   ------------------------------- -------- 10.0/12.7 MB 313.2 kB/s eta 0:00:09\n",
      "   -------------------------------- ------- 10.2/12.7 MB 316.9 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 10.5/12.7 MB 327.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 10.5/12.7 MB 327.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 10.5/12.7 MB 327.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 10.7/12.7 MB 330.2 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 10.7/12.7 MB 330.2 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 10.7/12.7 MB 330.2 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 332.7 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 332.7 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 341.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 341.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 341.5 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 11.5/12.7 MB 344.4 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 11.5/12.7 MB 344.4 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 11.5/12.7 MB 344.4 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 11.8/12.7 MB 350.3 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 11.8/12.7 MB 350.3 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 12.1/12.7 MB 353.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.1/12.7 MB 353.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.1/12.7 MB 353.0 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.7 MB 357.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.7 MB 357.7 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.7 MB 357.7 kB/s eta 0:00:02\n",
      "   ---------------------------------------  12.6/12.7 MB 359.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 361.7 kB/s eta 0:00:00\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 493.7 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.5/6.3 MB 493.7 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 516.5 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 516.5 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.0/6.3 MB 524.3 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.0/6.3 MB 524.3 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.0/6.3 MB 524.3 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.0/6.3 MB 524.3 kB/s eta 0:00:10\n",
      "   -------- ------------------------------- 1.3/6.3 MB 453.5 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 1.3/6.3 MB 453.5 kB/s eta 0:00:11\n",
      "   -------- ------------------------------- 1.3/6.3 MB 453.5 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 436.9 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 436.9 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 436.9 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 437.8 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 437.8 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 2.1/6.3 MB 448.3 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 2.1/6.3 MB 448.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 2.4/6.3 MB 459.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 2.4/6.3 MB 459.7 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 473.4 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 473.4 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 2.9/6.3 MB 486.4 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 2.9/6.3 MB 486.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.1/6.3 MB 494.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 3.1/6.3 MB 494.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 3.4/6.3 MB 502.1 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.4/6.3 MB 502.1 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 3.4/6.3 MB 502.1 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 503.8 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 503.8 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 3.9/6.3 MB 511.7 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 4.2/6.3 MB 521.0 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.2/6.3 MB 521.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 529.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 529.5 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 4.7/6.3 MB 538.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 538.2 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 542.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 5.2/6.3 MB 548.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.3 MB 548.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 551.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 551.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.8/6.3 MB 555.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 555.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 555.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 552.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 552.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 552.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 542.2 kB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 372.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.5/5.4 MB 372.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.5/5.4 MB 372.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.5/5.4 MB 372.9 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 345.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 345.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 345.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 345.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 345.8 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 1.0/5.4 MB 308.8 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 1.0/5.4 MB 308.8 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 1.0/5.4 MB 308.8 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 1.3/5.4 MB 318.0 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 1.3/5.4 MB 318.0 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 1.3/5.4 MB 318.0 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 1.3/5.4 MB 318.0 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 327.7 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 327.7 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 1.8/5.4 MB 344.7 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 1.8/5.4 MB 344.7 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 1.8/5.4 MB 344.7 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 2.1/5.4 MB 359.1 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 2.1/5.4 MB 359.1 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 2.1/5.4 MB 359.1 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 362.7 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 362.7 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 362.7 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 362.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 2.6/5.4 MB 362.1 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 2.6/5.4 MB 362.1 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 2.6/5.4 MB 362.1 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 2.9/5.4 MB 367.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 2.9/5.4 MB 367.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 2.9/5.4 MB 367.9 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 368.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 368.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 368.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 368.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 3.4/5.4 MB 364.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 3.4/5.4 MB 364.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 3.4/5.4 MB 364.7 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 3.7/5.4 MB 365.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 3.7/5.4 MB 365.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 3.7/5.4 MB 365.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 3.7/5.4 MB 365.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 3.7/5.4 MB 365.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 358.1 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 4.2/5.4 MB 341.9 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 4.2/5.4 MB 341.9 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 4.2/5.4 MB 341.9 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 4.2/5.4 MB 341.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 4.5/5.4 MB 338.5 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 4.5/5.4 MB 338.5 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 4.5/5.4 MB 338.5 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 4.5/5.4 MB 338.5 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 4.5/5.4 MB 338.5 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 332.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 314.9 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 5.2/5.4 MB 297.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 297.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 297.6 kB/s eta 0:00:00\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.0 preshed-3.0.10 pydantic-2.11.5 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\elsha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\elsha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\elsha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy #this library is used for natural language processing tasks, such as tokenization, part-of-speech tagging, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m spacy download en_core_web_md # this line downloads the English language model for spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load spaCy model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define theme labels (you can adjust these)\n",
    "themes = ['Account Access Issues', 'Transaction Performance', 'User Interface & Experience', 'Customer Support', 'Feature Requests']\n",
    "\n",
    "# Embed themes\n",
    "theme_vectors = {theme: nlp(theme).vector for theme in themes}\n",
    "\n",
    "bank_theme_mapping = {}\n",
    "\n",
    "for bank, group in df.groupby('bank_name'):\n",
    "    # TF-IDF vectorization for current bank\n",
    "    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(group['review_text'].astype(str))\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Assign each keyword to the most similar theme\n",
    "    theme_count = {theme: 0 for theme in themes}\n",
    "    \n",
    "    for kw in keywords:\n",
    "        kw_vec = nlp(kw).vector\n",
    "        if np.linalg.norm(kw_vec) == 0:\n",
    "            continue  # skip empty vectors (rare but possible)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = {theme: np.dot(kw_vec, vec) / (np.linalg.norm(vec) * np.linalg.norm(kw_vec)) for theme, vec in theme_vectors.items()}\n",
    "        best_theme = max(similarities, key=similarities.get)\n",
    "        theme_count[best_theme] += 1\n",
    "    \n",
    "    # Get top 3‚Äì5 themes\n",
    "    sorted_themes = sorted(theme_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    bank_theme_mapping[bank] = [theme for theme, count in sorted_themes if count > 0][:5]\n",
    "\n",
    "# Output themes per bank\n",
    "for bank, top_themes in bank_theme_mapping.items():\n",
    "    print(f\"{bank}: {top_themes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script preprocessing (tokenization, stop-word removal, lemmatization if useful) with Pandas and NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize #for text preprocessing, which includes tokenization, stopword removal, and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer #for text preprocessing, which includes tokenization, stopword removal, and lemmatization\n",
    " \n",
    "#sentiment analysis\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #for sentiment analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #for text vectorization, means converting text data into numerical format that machine learning algorithms can understand, what TF-IDF does is that it takes into account both the frequency of words and their importance in the context of the document, which can help the model understand the meaning and relevance of words in the text.\n",
    "from sklearn.model_selection import train_test_split #for splitting the dataset into training and testing sets\n",
    "from sklearn.naive_bayes import MultinomialNB #for building a Naive Bayes classifier, which is a simple and effective algorithm for text classification tasks\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#keyword Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud #for visualizing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\elsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt') # for tokenization, means splitting text into words\n",
    "nltk.download('stopwords') # for removing common words like 'the', 'is', etc.\n",
    "nltk.download('averaged_perceptron_tagger') # for part-of-speech tagging, meaning identifying the grammatical category of words\n",
    "nltk.download('vader_lexicon') # for sentiment analysis using VADER\n",
    "nltk.download('wordnet') # for lemmatization, which is reducing words to their base form, eg. 'running' to 'run'\n",
    "nltk.download('omw-1.4') # for lemmatization with WordNet, which is a lexical database for the English language\n",
    "nltk.download('punkt') # for tokenization, which is splitting text into sentences or words\n",
    "nltk.download('punkt_tab') # for tokenization with tab-separated values, which is useful for processing text data in a tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           really happy app siple use everything\n",
      "1                       liked app user interface basic attractive\n",
      "2    atm support transfer like country kenya nigeria south africa\n",
      "3                                                     app problem\n",
      "4                                   app proactive good connection\n",
      "Name: processed_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# def preprocess_text(text):\n",
    "#     # Tokenization\n",
    "#     tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "#     # Remove stopwords\n",
    "#     lematize = WordNetLemmatizer()\n",
    "\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#     tokens = [lematize.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "#     return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "\n",
    "# df[\"processed_review_text\"] = df[\"review_text\"].apply(preprocess_text)\n",
    "\n",
    "# Preprocessing function\n",
    "\n",
    "# This function takes a text input, tokenizes it, removes stopwords, and lemmatizes the words.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Tokenize and convert to lowercase\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer, which reduces words to their base form\n",
    "    stop_words = set(stopwords.words('english')) # Set of common words to remove, like 'the', 'is', etc.\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words] # Remove non-alphanumeric tokens and stopwords\n",
    "    return ' '.join(tokens) # Join tokens back into a single string\n",
    "\n",
    "# Apply preprocessing       \n",
    "df['processed_review'] = df['review_text'].apply(preprocess_text) # Display the first few rows of the processed reviews\n",
    "\n",
    "print(df['processed_review'].head(5)) # Display the first few rows of the processed reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Feature Requests, Account Access Issues, Transaction Performance, User Interface & Experience, Customer Support\n",
       "1    Feature Requests, Account Access Issues, Transaction Performance, User Interface & Experience, Customer Support\n",
       "2    Feature Requests, Account Access Issues, Transaction Performance, User Interface & Experience, Customer Support\n",
       "3    Feature Requests, Account Access Issues, Transaction Performance, User Interface & Experience, Customer Support\n",
       "4    Feature Requests, Account Access Issues, Transaction Performance, User Interface & Experience, Customer Support\n",
       "Name: identified_theme, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column named identified_theme\n",
    "df['identified_theme'] = df['bank_name'].apply(lambda x: ', '.join(bank_theme_mapping[x])) # Apply the lambda function to create the identified_theme column,\n",
    "#bank_theme_mapping[x]): this retrieves the top themes for each bank from the bank_theme_mapping dictionary and joins them into a single string separated by commas.\n",
    "#bank theme mapping is a dictionary where each key is a bank name and each value is a list of identified themes for that bank.\n",
    "\n",
    "\n",
    "# Define the columns you want to save\n",
    "columns_to_save = ['review_text', 'sentiment_label', 'sentiment_score', 'identified_theme']\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df[columns_to_save].to_csv('results.csv', index=False)\n",
    "\n",
    "df['identified_theme'].head(5)  # Display the first few rows of the identified themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords: ['account' 'after' 'all' 'am' 'amazing' 'an' 'and' 'app' 'application'\n",
      " 'apps' 'are' 'as' 'at' 'bank' 'banking' 'be' 'best' 'but' 'by' 'can'\n",
      " 'cbe' 'developer' 'do' 'doesn' 'don' 'easy' 'ethiopia' 'even' 'ever'\n",
      " 'every' 'fast' 'fix' 'for' 'from' 'get' 'good' 'great' 'has' 'have' 'if'\n",
      " 'in' 'is' 'it' 'its' 'like' 'make' 'me' 'mobile' 'money' 'more' 'my'\n",
      " 'need' 'new' 'nice' 'no' 'not' 'of' 'on' 'one' 'only' 'option' 'or'\n",
      " 'other' 'please' 'problem' 'really' 'service' 'simple' 'so' 'some'\n",
      " 'sometimes' 'system' 'thank' 'thanks' 'that' 'the' 'there' 'this' 'time'\n",
      " 'to' 'transaction' 'transactions' 'transfer' 'up' 'update' 'use' 'using'\n",
      " 'very' 'was' 'we' 'what' 'when' 'why' 'with' 'work' 'working' 'wow' 'you'\n",
      " 'your' '·äê·ãç']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the dataset\n",
    "\n",
    "df_results = pd.read_csv('results.csv')  # Read the results CSV file\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(df_results['review_text'].astype(str))  # Convert review_text to string and fit the vectorizer\n",
    "\n",
    "# Get top keywords\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "print(\"Top Keywords:\", keywords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster into 3‚Äì5 themes per bank (e.g., UI, reliability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
